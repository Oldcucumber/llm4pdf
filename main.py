# ----------------------- é…ç½®åŒºåŸŸ -----------------------
API_ENDPOINT = "http://100.64.1.1:8000/v1/chat/completions"
PDF_INPUT_DIR = "InputPDF"
OUTPUT_DIR = "Results"
CONTEXT_WINDOW = 12800 # ä¸Šä¸‹æ–‡çª—å£å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰
REQUEST_TIMEOUT = 120  # è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
TARGET_SUMMARY_LENGTH = 5000         # æœŸæœ›ç”Ÿæˆçš„æœ€ç»ˆç»¼è¿°çš„ç›®æ ‡é•¿åº¦ï¼ˆä»¥å­—ç¬¦è®¡ï¼‰
INITIAL_MAX_TOKENS = TARGET_SUMMARY_LENGTH*0.25  # åˆå§‹çš„ MAX_TOKENS å€¼
DYNAMIC_ADJUSTMENT_INTENSITY = 0.08   # åŠ¨æ€è°ƒæ•´ token çš„å¼ºåº¦ (éœ€è¦æ ¹æ®å®éªŒè°ƒæ•´)
DYNAMIC_MAX_TOKENS_LOWER_BOUND = 500   # åŠ¨æ€è®¡ç®—å‡ºçš„ MAX_TOKENS çš„æœ€å°å€¼
MAX_TOKENS = TARGET_SUMMARY_LENGTH*1.2                  # API å“åº”ä¸­æœŸæœ›è¿”å›çš„æœ€å¤§ token æ•° (ä¸Šé™)
# ------------------------------------------------------

import json
import re
from pathlib import Path
from datetime import datetime
from PyPDF2 import PdfReader
import requests
import nltk
from nltk.tokenize import sent_tokenize

# ç¡®ä¿ NLTK punkt tokenizer å·²ä¸‹è½½ï¼Œå¤©çŸ¥é“è¿™ç©æ„ä¸ºä»€ä¹ˆè¿˜æœ‰å¤–ç½®åº“
try:
    nltk.data.find('tokenizers/punkt')
except nltk.downloader.DownloaderError:
    nltk.download('punkt')

class IterativeSummarizer:
    """
    è¿­ä»£å¼æ–‡æœ¬æ‘˜è¦ç”Ÿæˆå™¨ï¼ŒåŠ¨æ€æå–æ–‡æœ¬ï¼Œä½¿ç”¨ NLTK è¿›è¡Œå¥å­åˆ†å‰²ï¼Œå¹¶æ˜¾ç¤ºå¤„ç†è¿›åº¦ã€‚
    """
    FINAL_SUMMARY_PROMPT = """ä½ æ˜¯ä¸€ä½èµ„æ·±ç ”ç©¶å‘˜ï¼Œè¯·æ ¹æ®ä»¥ä¸‹æä¾›çš„ç»¼è¿°ï¼Œç”Ÿæˆä¸€ä¸ªé«˜åº¦æ¦‚æ‹¬æ€§çš„æ€»ç»“ï¼Œçªå‡ºæœ€é‡è¦çš„å‘ç°ã€ç»“è®ºå’Œåˆ›æ–°ç‚¹ã€‚è¯·ä½¿ç”¨ç®€æ´æ˜äº†çš„è¯­è¨€ï¼Œå¹¶å°½é‡æ§åˆ¶åœ¨ä¸€å®šçš„å­—æ•°å†…ï¼ˆä¾‹å¦‚ï¼Œä¸è¶…è¿‡ 500 å­—ï¼‰ã€‚"""

    def __init__(self, target_summary_length):
        """
        åˆå§‹åŒ–ä¼šè¯ï¼Œè®¾ç½®è¯·æ±‚å¤´ã€‚
        """
        self.session = requests.Session()
        self.session.headers.update({"Content-Type": "application/json"})
        self.processed_length = 0  # è®°å½•å·²å¤„ç†çš„æ–‡æœ¬é•¿åº¦
        self.full_text = ""
        self.TARGET_SUMMARY_LENGTH = target_summary_length
        self.total_summary_characters = 0
        self.total_summary_tokens = 0
        self.dynamic_chars_per_token = 3.0  # åˆå§‹ä¼°è®¡å€¼
        self.integral_error = 0
        self.previous_error = 0

    def _get_next_chunk(self, text: str, max_length: int) -> str:
        """
        ä»æ–‡æœ¬ä¸­åŠ¨æ€æå–ä¸è¶…è¿‡æœ€å¤§é•¿åº¦çš„ä¸‹ä¸€ä¸ªæ–‡æœ¬å—ï¼ŒåŸºäºå¥å­ã€‚

        Args:
            text: åŸå§‹æ–‡æœ¬ã€‚
            max_length: å…è®¸æå–çš„æœ€å¤§é•¿åº¦ã€‚

        Returns:
            æå–çš„ä¸‹ä¸€ä¸ªæ–‡æœ¬å—ã€‚
        """
        start_index = self.processed_length
        remaining_text = text[start_index:]
        if not remaining_text:
            return ""

        sentences = sent_tokenize(remaining_text)
        extracted_chunk = ""
        current_length = 0

        for sentence in sentences:
            sentence_length = len(sentence) + 1  # +1 

            if current_length + sentence_length <= max_length:
                if extracted_chunk:
                    extracted_chunk += " " + sentence
                else:
                    extracted_chunk = sentence
                current_length += sentence_length
            else:
                break  # å¥å­è¶…å‡ºæœ€å¤§é•¿åº¦

        return extracted_chunk

    def _api_call(self, context: str, current_chunk: str, previous_summary_length: int, max_tokens: int) -> str:
        """
        è°ƒç”¨ API æ¥å£è¿›è¡Œæ–‡æœ¬æ‘˜è¦ï¼Œå¹¶åœ¨ Prompt ä¸­å¼•å¯¼æ¨¡å‹æ§åˆ¶ç»¼è¿°é•¿åº¦å’Œå¤„ç†è¿›åº¦ã€‚
        """
        remaining_length = len(self.full_text) - self.processed_length
        total_length = len(self.full_text)

        system_prompt_content = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ç§‘ç ”åŠ©ç†ï¼Œä»»åŠ¡æ˜¯æ ¹æ®æä¾›çš„å…ˆå‰æ‘˜è¦å’Œå½“å‰å†…å®¹ï¼Œç”Ÿæˆä¸€ä»½æ•´åˆçš„ç§‘ç ”ç»¼è¿°ã€‚è¿™æ˜¯ä¸€ä¸ªè¿­ä»£å¼çš„è¿‡ç¨‹ã€‚

**ç›®æ ‡ï¼š** ç”Ÿæˆçš„ç»¼è¿°çš„é•¿åº¦å°½é‡æ¥è¿‘ {self.TARGET_SUMMARY_LENGTH} å­—ç¬¦ã€‚

**ä¸Šæ¬¡ç»¼è¿°é•¿åº¦ï¼š** {previous_summary_length} å­—ç¬¦ã€‚

**å¤„ç†è¿›åº¦ï¼š** å½“å‰å¤„ç†åˆ°æ–‡æ¡£çš„ {self.processed_length} å­—ç¬¦ï¼Œå‰©ä½™ {remaining_length} å­—ç¬¦ï¼Œæ€»å…± {total_length} å­—ç¬¦ã€‚

è¯·å°†å½“å‰å†…å®¹çš„å…³é”®ä¿¡æ¯æ•´åˆåˆ°å…ˆå‰æ‘˜è¦ä¸­, å½¢æˆä¸€ä¸ªä¸æ–­å®Œå–„å’Œæ‰©å±•çš„ç»¼è¿°ã€‚è¯·æ³¨æ„ä»¥ä¸‹è¦æ±‚ï¼š

1.  **æ ¼å¼:** ä½¿ç”¨ Markdown æ ¼å¼ã€‚
2.  **å…³é”®ç»†èŠ‚:** ä¿ç•™æ‰€æœ‰å…³é”®çš„æŠ€æœ¯ç»†èŠ‚å’Œå®éªŒæ•°æ®ã€‚
3.  **åˆ›æ–°ç‚¹æ ‡æ³¨:** åœ¨åˆ›æ–°ç‚¹ä¹‹åç”¨ `[åˆ›æ–°]` æ ‡æ³¨ã€‚
4.  **æ–¹æ³•å¯¹æ¯”:** æ˜ç¡®æŒ‡å‡ºä¸åŒæ–¹æ³•ä¹‹é—´çš„å…³é”®å·®å¼‚ã€‚
5.  **æ•´åˆ:** é¿å…é‡å¤ï¼Œä¿æŒé€»è¾‘è¿è´¯ã€‚å¦‚æœå½“å‰å†…å®¹ä¸å…ˆå‰æ‘˜è¦å†²çªï¼Œä»¥å½“å‰å†…å®¹ä¸ºå‡†ã€‚

**è¯·æ ¹æ®ä»¥ä¸Šç›®æ ‡å’Œè¦æ±‚ç”Ÿæˆæ–°çš„ç»¼è¿°ã€‚**
"""

        payload = {
            "messages": [
                {
                    "role": "system",
                    "content": system_prompt_content
                },
                {
                    "role": "user",
                    "content": f"å…ˆå‰æ‘˜è¦ï¼š{context}\n\nå½“å‰å†…å®¹ï¼š{current_chunk}"
                }
            ],
            "max_tokens": max_tokens
        }

        try:
            response = self.session.post(
                API_ENDPOINT,
                data=json.dumps(payload),
                timeout=REQUEST_TIMEOUT
            )
            response.raise_for_status()
            response_json = response.json()
            summary = response_json['choices'][0]['message']['content']
            if 'usage' in response_json and 'total_tokens' in response_json['usage']:
                tokens_used = response_json['usage']['total_tokens']
                self.total_summary_tokens += tokens_used
                self.total_summary_characters = len(context) + len(summary) # è¿‘ä¼¼è®¡ç®—
                if self.total_summary_tokens > 0:
                    self.dynamic_chars_per_token = self.total_summary_characters / self.total_summary_tokens
            return summary
        except requests.exceptions.RequestException as e:
            print(f"APIè¯·æ±‚å¤±è´¥: {e}")
            return context

    def process(self, text: str) -> str:
        """
        è¿­ä»£å¤„ç†æ–‡æœ¬ï¼Œä½¿ç”¨åŠ¨æ€ token/å­—ç¬¦æ¯”ä¾‹å’Œ PID æ§åˆ¶ï¼ˆç®€åŒ–ä¸ºæ¯”ä¾‹æ§åˆ¶ï¼‰è°ƒæ•´ MAX_TOKENSã€‚
        """
        self.full_text = text
        accumulated_summary = ""
        self.processed_length = 0
        total_length = len(text)

        while self.processed_length < total_length:
            available_space = CONTEXT_WINDOW - len(accumulated_summary) - 200
            next_chunk = self._get_next_chunk(text, available_space)

            if not next_chunk:
                break

            chunk_length = len(next_chunk)
            print(f"å¤„ç†æ–‡æœ¬å— (é•¿åº¦: {chunk_length})ï¼Œå·²å¤„ç†: {self.processed_length}/{total_length}", end=" ")

            target_tokens = TARGET_SUMMARY_LENGTH / self.dynamic_chars_per_token if TARGET_SUMMARY_LENGTH > 0 and self.dynamic_chars_per_token > 0 else MAX_TOKENS
            current_summary_tokens = len(accumulated_summary) // self.dynamic_chars_per_token if self.dynamic_chars_per_token > 0 else 0

            # ç®€å•çš„æ¯”ä¾‹æ§åˆ¶,PIDå¤§æˆåŠŸ
            error = target_tokens - current_summary_tokens
            adjustment = error * DYNAMIC_ADJUSTMENT_INTENSITY
            dynamic_max_tokens = int(INITIAL_MAX_TOKENS + adjustment)

            dynamic_max_tokens = min(MAX_TOKENS, max(DYNAMIC_MAX_TOKENS_LOWER_BOUND, dynamic_max_tokens))

            progress = (self.processed_length / total_length) * 100
            bar_length = 40
            filled_length = int(bar_length * progress / 100)
            bar = 'â–ˆ' * filled_length + '-' * (bar_length - filled_length)
            print(f"[{bar}] {progress:.2f}%")

            accumulated_summary = self._api_call(accumulated_summary, next_chunk, len(accumulated_summary), dynamic_max_tokens)
            self.processed_length += chunk_length

            print(f"å½“å‰æ‘˜è¦é•¿åº¦: {len(accumulated_summary)} å­—ç¬¦ (é¢„è®¡ token: {len(accumulated_summary) // self.dynamic_chars_per_token:.2f}), åŠ¨æ€ MAX_TOKENS: {dynamic_max_tokens:.2f}, åŠ¨æ€å­—ç¬¦/Tokenæ¯”ä¾‹: {self.dynamic_chars_per_token:.2f}\n{'-'*40}")

        return accumulated_summary

class PDFProcessor:
    """
    ç”¨äºåŠ è½½å’Œåˆå¹¶ PDF æ–‡ä»¶çš„æ–‡æœ¬å†…å®¹ã€‚
    """
    @staticmethod
    def load_and_merge() -> str:
        """
        åŠ è½½æŒ‡å®šç›®å½•ä¸‹æ‰€æœ‰ PDF æ–‡ä»¶çš„æ–‡æœ¬å†…å®¹ï¼Œå¹¶å°†å®ƒä»¬åˆå¹¶æˆä¸€ä¸ªå­—ç¬¦ä¸²ã€‚

        Returns:
            åˆå¹¶åçš„ PDF æ–‡æœ¬ã€‚
        """
        merged_text = []
        pdf_dir = Path(PDF_INPUT_DIR)
        for pdf_file in pdf_dir.glob("*.pdf"):
            try:
                with pdf_file.open("rb") as f:
                    text = "\n".join([
                        p.extract_text() or ""
                        for p in PdfReader(f).pages
                    ])
                    merged_text.append(f"[æ–‡ä»¶ï¼š{pdf_file.name}]\n{text}")
                    print(f"å·²åŠ è½½: {pdf_file.name}")
            except Exception as e:
                print(f"æ–‡ä»¶é”™è¯¯ {pdf_file.name}: {e}")
        return "\n\n".join(merged_text)

def main():
    """
    ä¸»ç¨‹åºå…¥å£ã€‚
    """
    # è¾“å‡ºç›®å½•
    output_dir = Path(OUTPUT_DIR)
    output_dir.mkdir(exist_ok=True)

    # å¤„ç†æµç¨‹
    print("\n" + "="*40)
    print("è¿­ä»£å¼è®ºæ–‡åˆ†æç³»ç»Ÿ")
    print("="*40)

    # 1. åŠ è½½PDF
    print("\nğŸ“‚ æ­£åœ¨åŠ è½½PDFæ–‡ä»¶...")
    full_text = PDFProcessor.load_and_merge()
    print(f"æ€»æ–‡æœ¬é•¿åº¦: {len(full_text)} å­—ç¬¦")

    # 2. ç”Ÿæˆæ‘˜è¦
    print("\nğŸ”¨ å¼€å§‹è¿­ä»£å¤„ç†...")
    summarizer = IterativeSummarizer(TARGET_SUMMARY_LENGTH)
    final_summary = summarizer.process(full_text)

    # 3. ä¿å­˜ç»“æœåˆ° Markdown æ–‡ä»¶
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_path = output_dir / f"ç»¼è¿°æŠ¥å‘Š_{timestamp}.md"

    report = f"# è¿­ä»£å¼è®ºæ–‡ç»¼è¿°\n\n" \
             f"**ç”Ÿæˆæ—¶é—´**: {datetime.now()}\n" \
             f"**å¤„ç†æ–‡ä»¶**: {len(list(Path(PDF_INPUT_DIR).glob('*.pdf')))} ç¯‡\n\n" \
             f"{final_summary}"

    output_path.write_text(report, encoding="utf-8")
    print(f"\nâœ… æŠ¥å‘Šå·²ä¿å­˜è‡³: {output_path}")

if __name__ == "__main__":
    main()