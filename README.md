# 大模型读书法 (实验性)

**重要提示：** 这只是一个我个人觉得有趣，和朋友随便搓出来玩的验证性测试程序。如果你打算把它用到正经的地方，请务必自己好好检查和修改！

## 原理

核心原理其实挺直白的：我们先用模型生成一部分内容的“读后感”或者说综述，然后把这部分“读后感”和原文接下来的内容，按照一定的比例再一起扔给模型。这样一来，模型就像人类一样，可以一段一段地“读”下去了。

这个思路是人类阅读的方法的启发。

## 只是个玩具

我知道肯定有其他更牛X、更靠谱的办法来处理长文本。但这玩意儿的乐趣在于，它或许能让你用一张只有 6G 显卡的入门级显卡，也能拉起来一个 0.5B 这种规模的模型跑一跑，虽然效果可能没那么完美，But it's work。

## 前端还在难产中...

目前配套的前端还没弄出来，我们正在努力地搓一个简单的界面。

## 代码嘛...嗯...

代码质量就那样了，挺烂的。如果有哪位路过的大佬看到了，觉得有什么可以优化或者改进的地方，请狠狠地指出问题！欢迎斧正！

## 欢迎一起玩！

如果你觉得这个思路有点意思，或者有什么想法、建议，欢迎随时提 issue 或者 pull request 。
